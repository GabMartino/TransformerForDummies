

seed: 42
##### DATASET PARAMETERS
source_language:
  name: English
  dataset_path: "./Datasets/it-en/europarl-v7.it-en.en"
  vocabulary_path: "./Dataloaders/english_vocab.json"

target_language:
  name: Italian
  dataset_path: "./Datasets/it-en/europarl-v7.it-en.it"
  vocabulary_path: "./Dataloaders/italian_vocab.json"
maximum_dataset_lenght: 100000

###### ENCODER
encoder:
  vocab_size: 10000
  ## Actually not used. Extracted from the dataset
  num_layers: 1
  ff_hidden_size: 128

#### DECODER
decoder:
  vocab_size: 10000
  ## Actually not used. Extracted from the dataset
  num_layers: 1
  ff_hidden_size: 128

#### BOTH
same_source_target_vocabulary: False
## If True the embedding layer will be shared between the encoder and decoder, otherwise the encoder layers will be different
embedding_size: 16
num_heads: 1
ignore_index: None ## Extracted from the dataset class

### TRAINING CONFIG
checkpoint_dir: "./checkpoints"
logs_dir: "./logs"
from_checkpoint: True
train: False
test: True

max_epochs: 1
lr: 1e-3
label_smoothing: 0.1
with_scheduler: False ## False just use the learning rate, True use the scheduler and learning rate like in the original article
batch_size: 6
dropout: 0.1
warmup_steps: 4000
